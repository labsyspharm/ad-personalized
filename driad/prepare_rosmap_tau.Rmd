---
title: "DRIAD Tau signature"
author: "Clemens Hug"
output: html_document
---


```{r setup}
library(tidyverse)
library(synExtra)
library(data.table)
library(powerjoin)
library(here)
library(qs)
library(batchtools)


synapser::synLogin()
syn <- synDownloader("~/data", .cache = TRUE)

data_dir <- file.path("driad", "additional_jaks")
dir.create(data_dir, showWarnings = FALSE, recursive = TRUE)
```


Requires `ordinal_regression` branch of DRIAD.
https://github.com/clemenshug/DRIAD/tree/ordinal_regression

```{r eval=FALSE}
remotes::install_github("clemenshug/DRIAD", ref = "ordinal_regression")
remotes::install_github("labsyspharm/ordinalRidge")
```

From Supplement 3 of https://pmc.ncbi.nlm.nih.gov/articles/PMC10516095/#SM1

```{r}
gene_signatures <- list(
  tau_signature_ad = c("NPIPA1", "GOLGA8A", "FIZ1", "EPC1", "NAT8L", "ETV3", "RPRD1B",
"IFITM2", "KRT28", "MEGF8", "CCDC88B", "APOBEC3A_B", "RPL34",
"FTH1", "VAMP2", "ATP2B1", "SPTAN1", "PEBP1", "STMN1", "SYT11",
"FKBP1A", "SNU13", "PGAM4", "PRNP", "PTMA", "DNAJA4", "TOMM20",
"G3BP2", "NNAT", "YWHAG", "SYT1", "CALM3", "RAB6A", "ATF4", "ATP6V1G2",
"PAFAH1B1", "TSPYL1", "YWHAZ", "YWHAH"),
  tau_signature_part = c(
    c("BTC", "PTBP1", "COL5A2", "TRAF3IP3", "PKD1", "PRELID3B", "IL4I1",
"KRTAP10-10", "PNO1", "CHGA", "CYP2A6", "SPIN2B", "POMZP3", "RPRD1B",
"HMGCLL1", "TXN2", "ACSF2", "PDE7A", "PNN", "TOMM5", "BTBD6",
"SARDH", "NRGN", "MFSD6L", "GALR3", "TMX3", "BASP1", "DUX5",
"APOBEC3A_B", "DUX4", "MAPKBP1", "PLD3", "CKB", "RPS16", "JOSD2",
"TTC33", "ZNF28", "TTYH3", "ST3GAL2", "NBPF15", "PGRMC2", "STMN3",
"UCHL1", "GRIA2", "TMEM30A", "SRSF5", "APP", "FTH1", "CLTC",
"BMERB1", "FXYD6", "CLSTN1", "VDAC3", "SPTAN1", "MTCH1", "SCAMP5",
"PEA15", "PPP2CB", "VSNL1", "MLF2", "ATP2B1", "YWHAQ", "TSPAN7",
"C1QL3", "PNMA2", "BSG", "ATP6V1E1", "MAP1B", "TSPYL1", "APLP2",
"NCKAP1", "CALM1")
  )
)
```



```{r}

rosmap_tdp43_classification <- syn("syn44277761") %>%
  read_csv()

rosmap_quant_clinical <- syn("syn44335073") %>%
  read_csv()

rosmap_quant_clinical_split <- rosmap_quant_clinical %>%
  mutate(
    across(-c(ID, brain_region, PMI, AOD, CDR, Braak), ~log10(.x + 1))
  ) %>%
  power_inner_join(
    distinct(rosmap_tdp43_classification, file, class_low, class_high),
    by = c("ID" = "file"),
    check = check_specs(
      duplicate_keys_left = "abort",
      duplicate_keys_right = "abort",
      unmatched_keys_left = "warn",
      unmatched_keys_right = "abort"
    )
  ) %>% {
    bind_rows(
      low_threshold = select(., -class_high) %>%
        rename(class = class_low),
      high_threshold = select(., -class_low) %>%
        rename(class = class_high),
      all = select(., -class_low, -class_high) %>%
        mutate(class = "all"),
      .id = "classification"
    )
  } %>%
  group_nest(brain_region, classification, class)
```



Re-use prediction tasks created for chromatin modifier run

```{r}
# qsave(
#   prediction_tasks_pairs, file.path(data_dir, "prediction_tasks.qs")
# )
prediction_tasks_chromatin_mod <- syn("syn64765801") %>%
  qread() %>%
  mutate(
    task_path = file.path(data_dir, paste0(task_id, ".qs"))
  )

gene_set_sizes <- map_int(gene_signatures, length) %>%
  unique()

gene_universe <- colnames(rosmap_quant_clinical) %>%
  setdiff(
    c("ID", "brain_region", "PMI", "AOD", "CDR",
      "Braak", "Barcode", "Label")
  )

setdiff(
  gene_signatures %>%
    reduce(union),
  gene_universe
)

```

Adding prediction tasks for the `all` classification that weren't present
in the chromatin modifier run.

```{r}
prepare_task <- \(x, task) {
  task_map <- list(
    all_pooled = c("0", "0", "0", "1", "1", "2", "2"),
    all = as.character(0:6)
  )
  # browser()
  mutate(
    x,
    Label = task_map[[task]][Braak + 1] %>%
      fct_inseq(ordered = TRUE)
  )
}

set.seed(42)
prediction_tasks <- rosmap_quant_clinical_split %>%
  crossing(
    comparison = c("all", "all_pooled")
  ) %>%
  # Only use conditions that worked best in cross-validation
  filter(
    brain_region == "PCC",
    classification == "all"
  ) %>%
  mutate(
    task = map2(data, comparison, prepare_task)
  ) %>%
  select(-data) %>%
  mutate(
    task_id = paste("prediction_task", brain_region, classification, class, comparison, sep = "_"),
    task_path = file.path(data_dir, paste0(task_id, ".qs")),
    pairs = map(task, DRIAD::preparePairs)
  ) %>%
  bind_rows(
    prediction_tasks_chromatin_mod
  )

qsave(
  prediction_tasks, file.path(data_dir, "prediction_tasks.qs")
)
# prediction_tasks <- qread(file.path(data_dir, "prediction_tasks.qs"))

pwalk(
  prediction_tasks,
  function(task, pairs, task_path, ...) {
    # browser()
    x <- list(task = task, pairs = pairs)
    qsave(
      x,
      task_path
      # compress = "xz"
    )
  }
)

```



```{r}
compound_gene_sets <- gene_signatures %>%
  enframe("gs_id", "genes") %>%
  mutate(
    gene_set_size = map_int(genes, length)
  )

```

```{r}

N_BACKGROUND_GENE_SETS <- 1000

set.seed(42)
background_gene_sets <- crossing(
  gene_set_size = gene_set_sizes,
  seq_id = seq_len(N_BACKGROUND_GENE_SETS)
) %>%
  mutate(
    genes = map(
      gene_set_size,
      \(x) sample(gene_universe, x, replace = FALSE)
    ),
    gs_id = paste0("background_", gene_set_size, "_", seq_id)
  )

qsave(
  background_gene_sets,
  file.path(data_dir, "background_gene_sets.qs")
)
background_gene_sets <- qread(file.path(data_dir, "background_gene_sets.qs"))

qsave(
  compound_gene_sets,
  file.path(data_dir, "compound_gene_sets.qs")
)
compound_gene_sets <- qread(file.path(data_dir, "compound_gene_sets.qs"))

combined_gene_sets <- bind_rows(
  compound = compound_gene_sets,
  background = background_gene_sets,
  .id = "gs_type"
)

# Enforce minimum gene set size of 10
# and use only high_threshold for now
combined_gene_sets_selected <- combined_gene_sets %>%
  filter(
    gene_set_size >= 10
  )

prediction_task_gene_set_pairs <- cross_join(
  prediction_tasks %>%
    select(-c(task, pairs)),
  combined_gene_sets_selected
) %>%
  # Only use all_pooled task
  filter(
    comparison == "all_pooled"
  )

qsave(
  prediction_task_gene_set_pairs,
  file.path(data_dir, "prediction_task_gene_set_pairs.qs")
)
prediction_task_gene_set_pairs <- qread(file.path(data_dir, "prediction_task_gene_set_pairs.qs"))

# Figuring out running time of each task / gene set size combination by
# running a few samples

set.seed(42)
prediction_task_gene_set_pairs_performance_selected <- prediction_task_gene_set_pairs %>%
  group_by(
    task_id, gene_set_size
  ) %>%
  slice_sample(n = 2) %>%
  ungroup() %>%
  mutate(
    gene_sets = map2(genes, gs_id, \(x, y) set_names(list(dummy = x), y))
  )

run_pred_job <- function(task_path, gene_sets, ...) {
  library(tidyverse)
  library(furrr)
  library(DRIAD)
  library(qs)
  library(here)

  plan(sequential)

  message("Reading task...", task_path)
  task <- qread(task_path)
  message("Evaluating gene sets...", length(gene_sets))
  res <- tryCatch({
    DRIAD::evalGeneSets(
      gene_sets,
      XY = task[["task"]],
      lP = task[["pairs"]],
      nBK = 0,
      method = "or"
    ) %>%
      dplyr::select(-Feats, -BK)
  },
  error = function(e) {
    warning(e)
    NULL
  }
  )
  message("Done...")
  res
}


reg <- makeRegistry(
  file.dir = here(paste0("registry_additional_jaks_run_time", gsub(" ", "_", Sys.time()))),
  seed = 1
)
# reg <- loadRegistry("reg  ")

batchMap(
  fun = run_pred_job,
  task_path = prediction_task_gene_set_pairs_performance_selected$task_path,
  gene_sets = prediction_task_gene_set_pairs_performance_selected$gene_sets,
  reg = reg
)

# run_bk_job(
#   task_id = prediction_tasks_gene_sets[1:5,][["id"]][[1]],
#   background_sets = prediction_tasks_gene_sets[1:5,][["background_sets"]][[1]],
#   background_task_id = prediction_tasks_gene_sets[1:5,][["background_task_id"]][[1]]
# )

job_table <- findJobs(reg = reg) %>%
  # Chunk jobs into a single array job
  mutate(chunk = 1)

submitJobs(
  job_table,
  resources = list(
    memory = "1gb",
    ncpus = 1L,
    partition = "short",
    walltime = 10*60,
    chunks.as.arrayjobs = TRUE
  )
)

running_times_raw <- getJobTable(findDone(reg = reg), reg = reg) %>%
  bind_cols(
    prediction_task_gene_set_pairs_performance_selected
  )

qsave(
  running_times_raw,
  file.path(data_dir, "running_times_raw.qs")
)

running_times <- running_times_raw %>%
  group_by(
    task_id, gene_set_size
  ) %>%
  summarize(
    est_running_time = mean(time.running),
    .groups = "drop"
  )

set.seed(42)
# Planning jobs so that each one should take ~1h
PLANNED_RUNNING_TIME <- 60*60
prediction_task_job_sets <- prediction_task_gene_set_pairs %>%
  power_inner_join(
    running_times,
    by = c("task_id", "gene_set_size"),
    check = check_specs(
      duplicate_keys_right = "warn",
      unmatched_keys_left = "warn",
      unmatched_keys_right = "warn"
    )
  ) %>%
  slice_sample(prop = 1) %>%
  group_by(task_id) %>%
  mutate(
    batch_id = floor(cumsum(unclass(est_running_time)) / PLANNED_RUNNING_TIME)
  ) %>%
  ungroup() %>%
  select(task_id, task_path, gs_id, batch_id, genes) %>%
  group_nest(task_id, task_path, batch_id) %>%
  mutate(
    gene_sets = map(data, \(x) set_names(x$genes, x$gs_id))
  )

qsave(
  prediction_task_job_sets, file.path(data_dir, "prediction_task_job_sets.qs")
)
# prediction_task_job_sets <- qread(file.path(data_dir, "prediction_task_job_sets.qs"))

reg <- makeRegistry(
  file.dir = here(paste0("registry_additional_jaks_", gsub(" ", "_", Sys.time()))),
  seed = 1
)
# reg <- loadRegistry("reg  ")

batchMap(
  fun = run_pred_job,
  task_path = prediction_task_job_sets$task_path,
  gene_sets = prediction_task_job_sets$gene_sets,
  reg = reg
)

job_table <- findJobs(reg = reg) %>%
  # Chunk jobs into a single array job
  mutate(chunk = 1)

submitJobs(
  job_table,
  resources = list(
    memory = "1gb",
    ncpus = 1L,
    partition = "short",
    walltime = 100*60,
    chunks.as.arrayjobs = TRUE
  ),
  reg = reg
)

# Check if errored jobs contain compound gene sets
prediction_task_job_sets %>%
  slice(
    # findErrors()$job.id
    -findDone(reg = reg)$job.id
  ) %>%
  select(-gene_sets) %>%
  unnest(data) %>%
  filter(str_detect(gs_id, "compound"))
# Nope!

all_res_raw <- reduceResultsDataTable(
  findDone(reg = reg), reg = reg
)

qsave(
  all_res_raw,
  file.path(data_dir, "all_res_raw.qs")
)
all_res_raw <- qread(file.path(data_dir, "all_res_raw.qs"))

all_res_joined <- all_res_raw %>%
  unnest(result) %>%
  rename(gs_id = Set) %>%
  power_inner_join(
    prediction_task_job_sets %>%
      transmute(
        job.id = seq_len(n()),
        task_id
      ),
    by = c("job.id"),
    check = check_specs(
      duplicate_keys_right = "warn",
      unmatched_keys_left = "warn",
      unmatched_keys_right = "warn"
    )
  ) %>%
  power_inner_join(
    prediction_task_gene_set_pairs %>%
      select(-c(genes, task_path)),
    by = c("task_id", "gs_id"),
    check = check_specs(
      duplicate_keys_left = "warn",
      duplicate_keys_right = "warn",
      unmatched_keys_left = "warn",
      unmatched_keys_right = "warn"
    )
  )

fwrite(
  select(all_res_joined, where(negate(is.list))),
  file.path(data_dir, "compound_and_background_res.csv.gz")
)



syn_dir <- synMkdir("syn45053114", "Tau")
synStoreMany(
  c(
    file.path(
      data_dir,
      c(
        "all_res_raw.qs",
        "compound_and_background_res.csv.gz",
        "background_gene_sets.qs",
        "prediction_tasks.qs",
        "prediction_task_gene_set_pairs.qs",
        "all_res_gene_sets.qs"
      )
    )
  ),
  syn_dir,
  forceVersion = FALSE
)



```

